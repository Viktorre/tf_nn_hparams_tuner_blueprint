{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsHV-7cpVkyK"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np # for math and arrays\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "#import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np # for math and arrays\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "\n",
        "titanic_features = titanic.copy()\n",
        "titanic_labels = titanic_features.pop('survived')\n",
        "inputs = {}\n",
        "\n",
        "for name, column in titanic_features.items():\n",
        "  dtype = column.dtype\n",
        "  if dtype == object:\n",
        "    dtype = tf.string\n",
        "  else:\n",
        "    dtype = tf.float32\n",
        "\n",
        "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
        "\n",
        "numeric_inputs = {name:input for name,input in inputs.items()\n",
        "                  if input.dtype==tf.float32}\n",
        "\n",
        "x = tf.keras.layers.Concatenate()(list(numeric_inputs.values()))\n",
        "norm = tf.keras.layers.Normalization()\n",
        "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
        "all_numeric_inputs = norm(x)\n",
        "preprocessed_inputs = [all_numeric_inputs]\n",
        "\n",
        "for name, input in inputs.items():\n",
        "  if input.dtype == tf.float32:\n",
        "    continue\n",
        "  lookup = tf.keras.layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
        "  one_hot = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
        "\n",
        "  x = lookup(input)\n",
        "  x = one_hot(x)\n",
        "  preprocessed_inputs.append(x)\n",
        "\n",
        "preprocessed_inputs_cat = tf.keras.layers.Concatenate()(preprocessed_inputs)\n",
        "\n",
        "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
        "\n",
        "titanic_features_dict = {name: np.array(value) \n",
        "                         for name, value in titanic_features.items()}\n",
        "\n",
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "titanic_preprocessing(features_dict)\n",
        "x_train, x_valid, x_test = {}, {}, {}\n",
        "for col in pd.DataFrame(titanic_features_dict):\n",
        "  x_train[col] = np.array(pd.DataFrame(titanic_features_dict)[col][:500].values)\n",
        "  x_valid[col] = np.array(pd.DataFrame(titanic_features_dict)[col][500:563].values)\n",
        "  x_test[col] = np.array(pd.DataFrame(titanic_features_dict)[col][563:].values)\n",
        "y_train, y_valid, y_test = titanic_labels[:500],titanic_labels[500:563],titanic_labels[563:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128]))\n",
        "# HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([1]))\n",
        "# # HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
        "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "# HP_LR = hp.HParam('lr', hp.Discrete([0.001]))\n",
        "# HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1,64]))\n",
        "# METRIC_ACCURACY = 'accuracy'\n",
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([32,64,128,256,1000,2000]))\n",
        "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([1,2,3,4]))\n",
        "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "HP_LR = hp.HParam('lr', hp.Discrete([0.1,0.01,0.001,0.0001,]))\n",
        "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([1,16,32,64,128,256]))\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "\n",
        "log_name = 'logs_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'/hparam_tuning'\n",
        "\n",
        "with tf.summary.create_file_writer(log_name).as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_NUM_LAYERS, HP_OPTIMIZER, HP_LR, HP_BATCH_SIZE],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )\n",
        "\n",
        "def helper_fct_return_optimizer_w_learn_rate(opt_name:str,lr:float):\n",
        "  if opt_name == \"sgd\":\n",
        "    return tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "  if opt_name == \"adam\":\n",
        "    return tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  return \"error\"\n",
        "\n",
        "def train_test_model(hparams,run_dir,preprocessing_head, inputs):\n",
        "  body = tf.keras.models.Sequential()\n",
        "  for _ in range(int(hparams[HP_NUM_LAYERS])):\n",
        "    body.add(tf.keras.layers.Dense(hparams[HP_NUM_UNITS]))\n",
        "      # tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
        "  body.add(tf.keras.layers.Dense(1))\n",
        "  preprocessed_inputs = preprocessing_head(inputs)\n",
        "  result = body(preprocessed_inputs)\n",
        "  model = tf.keras.Model(inputs, result)\n",
        "  model.compile(\n",
        "      optimizer=helper_fct_return_optimizer_w_learn_rate(hparams[HP_OPTIMIZER],hparams[HP_LR],),\n",
        "      loss='BinaryCrossentropy', metrics=['accuracy'],)\n",
        "  model.fit(x_train, y_train,validation_data=(x_valid,y_valid),epochs=100, shuffle=True,verbose=False, callbacks=[ tf.keras.callbacks.TensorBoard(log_dir=run_dir+'_'+str(hparams[HP_NUM_LAYERS])+'layers_'+str(hparams[HP_NUM_UNITS])+'nodes_'+hparams[HP_OPTIMIZER]+str(hparams[HP_LR])+'_'+str(hparams[HP_BATCH_SIZE]), histogram_freq=1)],batch_size=(hparams[HP_BATCH_SIZE])) \n",
        "  _, accuracy = model.evaluate(x_valid, y_valid,verbose=False)\n",
        "  return accuracy\n",
        "\n",
        "def run(run_dir, hparams, preprocessing_head, inputs):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = train_test_model(hparams,run_dir, preprocessing_head, inputs)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
        "\n",
        "session_num = 0\n",
        "for num_units in tqdm(HP_NUM_UNITS.domain.values):\n",
        "  for num_layers in HP_NUM_LAYERS.domain.values:\n",
        "    for optimizer in HP_OPTIMIZER.domain.values:\n",
        "      for lr in HP_LR.domain.values:   \n",
        "        for batch_size in HP_BATCH_SIZE.domain.values:\n",
        "          hparams = {HP_NUM_UNITS: num_units,HP_NUM_LAYERS: num_layers, HP_OPTIMIZER: optimizer, HP_LR: lr, HP_BATCH_SIZE: batch_size }\n",
        "          run_name = \"run-%d\" % session_num\n",
        "          run(log_name+ run_name, hparams, titanic_preprocessing, inputs )\n",
        "          session_num += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hyperparameter_tuning_with_hparams.ipynb",
      "toc_visible": true
    },
    "interpreter": {
      "hash": "3f4dddeb97c43720685b63285ae9cf6c7cf66ba0658824cdc3274f63bac11808"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venvtb': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
